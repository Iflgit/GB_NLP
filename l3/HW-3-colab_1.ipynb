{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "view-in-github"
   },
   "source": [
    "<a href=\"https://colab.research.google.com/github/Iflgit/GB_NLP/blob/main/l3/HW-3-colab_1.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "SKtyWzdlv74n",
    "outputId": "0c06f605-db47-4631-db49-94ebbf275ba3"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Name: nltk\n",
      "Version: 3.6.2\n",
      "Summary: Natural Language Toolkit\n",
      "Home-page: http://nltk.org/\n",
      "Author: Steven Bird\n",
      "Author-email: stevenbird1@gmail.com\n",
      "License: Apache License, Version 2.0\n",
      "Location: c:\\users\\ifl\\documents\\github\\.venv\\lib\\site-packages\n",
      "Requires: regex, tqdm, click, joblib\n",
      "Required-by: \n",
      "---\n",
      "Name: gensim\n",
      "Version: 4.1.2\n",
      "Summary: Python framework for fast Vector Space Modelling\n",
      "Home-page: http://radimrehurek.com/gensim\n",
      "Author: Radim Rehurek\n",
      "Author-email: me@radimrehurek.com\n",
      "License: LGPL-2.1-only\n",
      "Location: c:\\users\\ifl\\documents\\github\\.venv\\lib\\site-packages\n",
      "Requires: numpy, Cython, smart-open, scipy\n",
      "Required-by: \n",
      "---\n",
      "Name: bokeh\n",
      "Version: 2.4.0\n",
      "Summary: Interactive plots and applications in the browser from Python\n",
      "Home-page: https://github.com/bokeh/bokeh\n",
      "Author: Bokeh Team\n",
      "Author-email: info@bokeh.org\n",
      "License: BSD-3-Clause\n",
      "Location: c:\\users\\ifl\\documents\\github\\.venv\\lib\\site-packages\n",
      "Requires: typing-extensions, packaging, Jinja2, tornado, pillow, PyYAML, numpy\n",
      "Required-by: \n",
      "---\n",
      "Name: pandas\n",
      "Version: 1.3.3\n",
      "Summary: Powerful data structures for data analysis, time series, and statistics\n",
      "Home-page: https://pandas.pydata.org\n",
      "Author: The Pandas Development Team\n",
      "Author-email: pandas-dev@python.org\n",
      "License: BSD-3-Clause\n",
      "Location: c:\\users\\ifl\\documents\\github\\.venv\\lib\\site-packages\n",
      "Requires: pytz, python-dateutil, numpy\n",
      "Required-by: statsmodels, seaborn, networkx, mapclassify, geopandas, category-encoders, catboost\n"
     ]
    }
   ],
   "source": [
    "!pip show nltk gensim bokeh pandas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "y6EAb5xAxUFD"
   },
   "outputs": [],
   "source": [
    "# !pip uninstall numpy\n",
    "# !pip install numpy\n",
    "# !pip show pandas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "wWzU9hqUomdU",
    "outputId": "0e0205a5-73da-4f43-c53e-4f890a723629"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: You are using pip version 21.1.3; however, version 21.2.4 is available.\n",
      "You should consider upgrading via the 'c:\\users\\ifl\\documents\\github\\.venv\\scripts\\python.exe -m pip install --upgrade pip' command.\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\Ifl\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\Ifl\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "!pip install -q --upgrade nltk gensim bokeh pandas\n",
    "\n",
    "import nltk\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "K-X7I7nc1gyS"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from nltk.tokenize import word_tokenize\n",
    "from gensim.models import Word2Vec\n",
    "from sklearn.manifold import TSNE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "UoYjVExHd_OC"
   },
   "outputs": [],
   "source": [
    "import re\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from nltk.corpus import stopwords\n",
    "from string import punctuation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xvqrFUS6vVhh"
   },
   "source": [
    "## Запилим пословный машинный перевод!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "1CXcr-ypzGXg",
    "outputId": "9a32bac6-f493-420f-94f4-65e4be2e9c47"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "'wget' is not recognized as an internal or external command,\n",
      "operable program or batch file.\n",
      "'wget' is not recognized as an internal or external command,\n",
      "operable program or batch file.\n",
      "'wget' is not recognized as an internal or external command,\n",
      "operable program or batch file.\n",
      "WARNING: You are using pip version 21.1.3; however, version 21.2.4 is available.\n",
      "You should consider upgrading via the 'c:\\users\\ifl\\documents\\github\\.venv\\scripts\\python.exe -m pip install --upgrade pip' command.\n"
     ]
    },
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'google.colab'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-6-5bab5f235952>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      8\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mpydrive\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mauth\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mGoogleAuth\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      9\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mpydrive\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdrive\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mGoogleDrive\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 10\u001b[1;33m \u001b[1;32mfrom\u001b[0m \u001b[0mgoogle\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcolab\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mauth\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     11\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0moauth2client\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mclient\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mGoogleCredentials\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     12\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'google.colab'"
     ]
    }
   ],
   "source": [
    "!wget -O ukr_rus.train.txt -qq --no-check-certificate \"https://drive.google.com/uc?export=download&id=1vAK0SWXUqei4zTimMvIhH3ufGPsbnC_O\"\n",
    "!wget -O ukr_rus.test.txt -qq --no-check-certificate \"https://drive.google.com/uc?export=download&id=1W9R2F8OeKHXruo2sicZ6FgBJUTJc8Us_\"\n",
    "!wget -O fairy_tale.txt -qq --no-check-certificate \"https://drive.google.com/uc?export=download&id=1sq8zSroFeg_afw-60OmY8RATdu_T1tej\"\n",
    "\n",
    "# Install the PyDrive wrapper & import libraries.\n",
    "# This only needs to be done once per notebook.\n",
    "!pip install -U -q PyDrive\n",
    "from pydrive.auth import GoogleAuth\n",
    "from pydrive.drive import GoogleDrive\n",
    "from google.colab import auth\n",
    "from oauth2client.client import GoogleCredentials\n",
    "\n",
    "# Authenticate and create the PyDrive client.\n",
    "# This only needs to be done once per notebook.\n",
    "auth.authenticate_user()\n",
    "gauth = GoogleAuth()\n",
    "gauth.credentials = GoogleCredentials.get_application_default()\n",
    "drive = GoogleDrive(gauth)\n",
    "\n",
    "downloaded = drive.CreateFile({'id': '1d7OXuil646jUeDS1JNhP9XWlZogv6rbu'})\n",
    "downloaded.GetContentFile('cc.ru.300.vec.zip')\n",
    "\n",
    "downloaded = drive.CreateFile({'id': '1yAqwqgUHtMSfGS99WLGe5unSCyIXfIxi'})\n",
    "downloaded.GetContentFile('cc.uk.300.vec.zip')\n",
    "\n",
    "!unzip cc.ru.300.vec.zip\n",
    "!unzip cc.uk.300.vec.zip"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7RqUeOXxws8y"
   },
   "source": [
    "Напишем простенькую реализацию модели машинного перевода.\n",
    "\n",
    "Идея основана на статье [Word Translation Without Parallel Data](https://arxiv.org/pdf/1710.04087.pdf). У авторов в репозитории еще много интересного: [https://github.com/facebookresearch/MUSE](https://github.com/facebookresearch/MUSE).\n",
    "\n",
    "А мы будем переводить с украинского на русский.\n",
    "\n",
    "![](https://raw.githubusercontent.com/yandexdataschool/nlp_course/master/resources/blue_cat_blue_whale.png)   \n",
    "*синій кіт* vs. *синій кит*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "jjPj9FTRry0U"
   },
   "outputs": [],
   "source": [
    "from gensim.models import KeyedVectors\n",
    "\n",
    "ru_emb = KeyedVectors.load_word2vec_format(\"cc.ru.300.vec\")\n",
    "uk_emb = KeyedVectors.load_word2vec_format(\"cc.uk.300.vec\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7rGx4TXWFJ65"
   },
   "source": [
    "Посмотрим на пару серпень-август (являющихся переводом)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "FkHer36xyh4n",
    "outputId": "1dfa222a-b7a3-4998-9533-59c430a06e90"
   },
   "outputs": [],
   "source": [
    "ru_emb.most_similar([ru_emb[\"август\"]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "1RSDixWvylEP",
    "outputId": "e2fac2b8-068f-4111-c6e5-290c1c598f43"
   },
   "outputs": [],
   "source": [
    "uk_emb.most_similar([uk_emb[\"серпень\"]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "iwmm3YQ1yl1U",
    "outputId": "354abfa7-216a-47ca-be64-b62a650e7820"
   },
   "outputs": [],
   "source": [
    "ru_emb.most_similar([uk_emb[\"серпень\"]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "lAsW7oxszE_I"
   },
   "outputs": [],
   "source": [
    "def load_word_pairs(filename):\n",
    "    uk_ru_pairs = []\n",
    "    uk_vectors = []\n",
    "    ru_vectors = []\n",
    "    with open(filename, \"r\", encoding='utf8') as inpf:\n",
    "        for line in inpf:\n",
    "            uk, ru = line.rstrip().split(\"\\t\")\n",
    "            if uk not in uk_emb or ru not in ru_emb:\n",
    "                continue\n",
    "            uk_ru_pairs.append((uk, ru))\n",
    "            uk_vectors.append(uk_emb[uk])\n",
    "            ru_vectors.append(ru_emb[ru])\n",
    "    return uk_ru_pairs, np.array(uk_vectors), np.array(ru_vectors)\n",
    "\n",
    "\n",
    "uk_ru_train, X_train, Y_train = load_word_pairs(\"ukr_rus.train.txt\")\n",
    "uk_ru_test, X_test, Y_test = load_word_pairs(\"ukr_rus.test.txt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9z6ts7DC0XmN"
   },
   "source": [
    "### Учим маппинг из одного пространства эмбеддингов в другое\n",
    "\n",
    "У нас есть пары слов, соответствующих друг другу, и их эмбеддинги. Найдем преобразование из одного пространства в другое, чтобы приблизить известные нам слова:\n",
    "\n",
    "$$W^*= \\arg\\min_W ||WX - Y||_F, \\text{где} ||*||_F - \\text{норма Фробениуса}$$\n",
    "\n",
    "Эта функция очень похожа на линейную регрессию (без биаса).\n",
    "\n",
    "**Задание** Реализуйте её - воспользуйтесь `LinearRegression` из sklearn с `fit_intercept=False`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "KRoevzlIzwhl"
   },
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LinearRegression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "WUBYJw78zr6p"
   },
   "outputs": [],
   "source": [
    "?LinearRegression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "fraTOQtu1YWI"
   },
   "outputs": [],
   "source": [
    "mapping = LinearRegression(fit_intercept=False).fit(X_train, Y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "PrzRk3ja1b_6"
   },
   "source": [
    "Проверим, куда перейдет `серпень`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Quax6HnF1aON",
    "outputId": "202e1408-dd86-409e-9e73-6099e3c82358"
   },
   "outputs": [],
   "source": [
    "august = mapping.predict(uk_emb[\"серпень\"].reshape(1, -1))\n",
    "ru_emb.most_similar(august)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "YRHzIbVBBHyy",
    "outputId": "3a98a19a-2807-471a-c53c-11d1e547ba65"
   },
   "outputs": [],
   "source": [
    "ru_emb.most_similar(mapping.predict(uk_emb[\"серпень\"].reshape(1, -1)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ih1GLNZt1nZX"
   },
   "source": [
    "Должно получиться, что в топе содержатся разные месяцы, но август не первый.\n",
    "\n",
    "Будем мерять percision top-k с k = 1, 5, 10.\n",
    "\n",
    "**Задание** Реализуйте следующую функцию:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Zspc2V894GPk",
    "outputId": "405f45c2-1a55-44fb-b5dd-0483816efedc"
   },
   "outputs": [],
   "source": [
    "# [item for item in august[:12] if 'август' in item]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "zkHUHGRP3uLq",
    "outputId": "4daaa526-fe5e-4923-f627-ef7909ee7348"
   },
   "outputs": [],
   "source": [
    "# 'август' in dict(ru_emb.most_similar(august)[:10]).keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "JnmrLp9y2gNI"
   },
   "outputs": [],
   "source": [
    "def precision(pairs, mapped_vectors, topn=1):\n",
    "    \"\"\"\n",
    "    :args:\n",
    "        pairs = list of right word pairs [(uk_word_0, ru_word_0), ...]\n",
    "        mapped_vectors = list of embeddings after mapping from source embedding space to destination embedding space\n",
    "        topn = the number of nearest neighbours in destination embedding space to choose from\n",
    "    :returns:\n",
    "        precision_val, float number, total number of words for those we can find right translation at top K.\n",
    "    \"\"\"\n",
    "    assert len(pairs) == len(mapped_vectors)\n",
    "    num_matches = 0\n",
    "    for i, (_, ru) in enumerate(pairs):\n",
    "      # print(topn, ru_emb.most_similar(mapped_vectors))\n",
    "      if ru in dict(ru_emb.most_similar(mapped_vectors)[:topn]).keys():\n",
    "        print('bingo')\n",
    "        num_matches += 1\n",
    "    precision_val = num_matches / len(pairs)\n",
    "    return precision_val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "-1NIvhSH2olG",
    "outputId": "f68476c2-0f80-4219-d009-b4c466926046"
   },
   "outputs": [],
   "source": [
    "assert precision([(\"серпень\", \"август\")], august, topn=5) == 0.0\n",
    "assert precision([(\"серпень\", \"август\")], august, topn=9) == 1.0\n",
    "assert precision([(\"серпень\", \"август\")], august, topn=10) == 1.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "M3RczbHmADXa",
    "outputId": "0ea898a0-26a5-41fe-e4a6-ab3a9683c4db"
   },
   "outputs": [],
   "source": [
    "ru_emb.most_similar(Y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "7wbPa-f_Ans8",
    "outputId": "1f3c066b-3afd-44e3-c4a8-cfbc7d3d3f22"
   },
   "outputs": [],
   "source": [
    "uk_ru_test[:15]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "YcwCPDL18kkO",
    "outputId": "5ef2dc4e-c872-4f09-cc17-035fe34197a4"
   },
   "outputs": [],
   "source": [
    "precision(uk_ru_test, Y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "SZwsBrCU9cHr",
    "outputId": "143be05d-cc48-4ab3-edce-28ff0919bf9c"
   },
   "outputs": [],
   "source": [
    "ru_emb.most_similar(Y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "8hyVKBt59zLC"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 178
    },
    "id": "9Ml_w1Tl2r7Y",
    "outputId": "66c74501-6a02-4f30-88cf-4f449657beb4"
   },
   "outputs": [],
   "source": [
    "assert precision(uk_ru_test, X_test) == 0.0\n",
    "assert precision(uk_ru_test, Y_test) == 1.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "-d9KQHMr2tx8"
   },
   "outputs": [],
   "source": [
    "precision_top1 = precision(uk_ru_test, mapping.predict(X_test), 1)\n",
    "precision_top5 = precision(uk_ru_test, mapping.predict(X_test), 5)\n",
    "\n",
    "assert precision_top1 >= 0.635\n",
    "assert precision_top5 >= 0.813"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "JNbDTP502urT"
   },
   "source": [
    "### Улучшаем маппинг\n",
    "\n",
    "Можно показать, что маппинг лучше строить ортогональным:\n",
    "$$W^*= \\arg\\min_W ||WX - Y||_F \\text{, где: } W^TW = I$$\n",
    "\n",
    "Искать его можно через SVD:\n",
    "$$X^TY=U\\Sigma V^T\\text{, singular value decompostion}$$\n",
    "\n",
    "$$W^*=UV^T$$\n",
    "\n",
    "**Задание** Реализуйте эту функцию."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "9de8XZ_F3v53"
   },
   "outputs": [],
   "source": [
    "def learn_transform(X_train, Y_train):\n",
    "    \"\"\" \n",
    "    :returns: W* : float matrix[emb_dim x emb_dim] as defined in formulae above\n",
    "    \"\"\"\n",
    "    <write code there>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "8WeCadzN382y"
   },
   "outputs": [],
   "source": [
    "W = learn_transform(X_train, Y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "p6qaMb0E3-f9"
   },
   "outputs": [],
   "source": [
    "ru_emb.most_similar([np.matmul(uk_emb[\"серпень\"], W)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "_Nn58crh4AH0"
   },
   "outputs": [],
   "source": [
    "assert precision(uk_ru_test, np.matmul(X_test, W)) >= 0.653\n",
    "assert precision(uk_ru_test, np.matmul(X_test, W), 5) >= 0.824"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "lqgcYk-c4DE5"
   },
   "source": [
    "### Пишем переводчик"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hwi70fP6FaAN"
   },
   "source": [
    "Реализуем простой пословный переводчик - для каждого слова будем искать его ближайшего соседа в общем пространстве эмбеддингов. Если слова нет в эмбеддингах - просто копируем его."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "0etAHUks4JOr"
   },
   "outputs": [],
   "source": [
    "with open(\"fairy_tale.txt\", \"r\") as in f:\n",
    "    uk_sentences = [line.rstrip().lower() for line in in f]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "JK_FJGmn4N7V"
   },
   "outputs": [],
   "source": [
    "def translate(sentence):\n",
    "    \"\"\"\n",
    "    :args:\n",
    "        sentence - sentence in Ukrainian (str)\n",
    "    :returns:\n",
    "        translation - sentence in Russian (str)\n",
    "\n",
    "    * find ukrainian embedding for each word in sentence\n",
    "    * transform ukrainian embedding vector\n",
    "    * find nearest russian word and replace\n",
    "    \"\"\"\n",
    "    <implement it!>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "H47pbFyk4P6D"
   },
   "outputs": [],
   "source": [
    "assert translate(\".\") == \".\"\n",
    "assert translate(\"1 , 3\") == \"1 , 3\"\n",
    "assert translate(\"кіт зловив мишу\") == \"кот поймал мышку\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "PAVWK7mE4RYU"
   },
   "outputs": [],
   "source": [
    "for sentence in uk_sentences:\n",
    "    print(\"src: {}\\ndst: {}\\n\".format(sentence, translate(sentence)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_5GrChTeFqIg"
   },
   "source": [
    "# Дополнительные материалы"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "HwffxpbmFwDh"
   },
   "source": [
    "## Почитать\n",
    "### База:  \n",
    "[On word embeddings - Part 1, Sebastian Ruder](http://ruder.io/word-embeddings-1/)  \n",
    "[Deep Learning, NLP, and Representations, Christopher Olah](http://colah.github.io/posts/2014-07-NLP-RNNs-Representations/)  \n",
    "\n",
    "### Как кластеризовать смыслы многозначных слов:  \n",
    "[Making Sense of Word Embeddings (2016), Pelevina et al](http://anthology.aclweb.org/W16-1620)    \n",
    "\n",
    "### Как оценивать эмбеддинги\n",
    "[Evaluation methods for unsupervised word embeddings (2015), T. Schnabel](http://www.aclweb.org/anthology/D15-1036)  \n",
    "[Intrinsic Evaluation of Word Vectors Fails to Predict Extrinsic Performance (2016), B. Chiu](https://www.aclweb.org/anthology/W/W16/W16-2501.pdf)  \n",
    "[Problems With Evaluation of Word Embeddings Using Word Similarity Tasks (2016), M. Faruqui](https://arxiv.org/pdf/1605.02276.pdf)  \n",
    "[Improving Reliability of Word Similarity Evaluation by Redesigning Annotation Task and Performance Measure (2016), Oded Avraham, Yoav Goldberg](https://arxiv.org/pdf/1611.03641.pdf)  \n",
    "[Evaluating Word Embeddings Using a Representative Suite of Practical Tasks (2016), N. Nayak](https://cs.stanford.edu/~angeli/papers/2016-acl-veceval.pdf)  \n",
    "\n",
    "\n",
    "## Посмотреть\n",
    "[Word Vector Representations: word2vec, Lecture 2, cs224n](https://www.youtube.com/watch?v=ERibwqs9p38)"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "include_colab_link": true,
   "name": "HW-3-colab.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
